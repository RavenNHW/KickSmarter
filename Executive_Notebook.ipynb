{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kickstart Informed: Executive Notebook\n",
    "\n",
    "This notebook can be run to go through the entire modeling process, _with the exception of the hyperparameter tuning, which can be found [here]('data_processing/GridSearch_CLOUD.ipynb')_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: What is Kickstarter? Why use machine learning? \n",
    "\n",
    "Kickstarter is a well-known crowdfunding website that was founded in 2009, with a focus on providing a platform for people fund new creations such as art, film, technology, and games of both the digital and physical variety.\n",
    "\n",
    "Largely, each individual project contains a large amount of objective data, following trends, new inventions and innovations, or other themes. On top of that, one may need to know a large amount of information such as when the project should be published, how long it should last, and the goal outside of just the manufacturing cost and shipping & handling. The more research it takes, the more time and energy one may have to put into that research that they could have instead spent working on the project.\n",
    "\n",
    "That's where Kickstart Informed comes in. With Machine Learning, one can add to their project to help increase the chance of success, without having to research quite the same amount of information. While it can't process things such as what the project itself is about and all of the intimate details that go along with it, it can take numerical, catagorical, binary, and date information to make some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with the Clean Data\n",
    "\n",
    "_If you would like to review the data cleaning process, go [here]('data/DataCleaningBook.ipynb')_\n",
    "\n",
    "As we can see here, we're starting with a total of 9 categories. Let's review them. \n",
    "\n",
    "- `country`: The country that the project creator resides in OR the country that the business is located in\n",
    "- `currency`: The original currency that the project used\n",
    "- `goal`: The project's goal in US Dollars\n",
    "- `staff_pick`: Whether or not the project was officially featured by Kickstarter\n",
    "- `category_parent_name` : The name of the parent category that the project was in\n",
    "- `start_month` : The month that the project began \n",
    "- `end_month` : The month that the project ended in\n",
    "- `project_length` : The total number of days that the project ran for\n",
    "- `blurb_len` : The length of the project's blurb\n",
    "\n",
    "The data has been split into train and test, with train being 80% of the dataset and test being 20%\n",
    "- Train: 138,192\n",
    "- Test: 34549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.src.modeling import *\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_train_test_data(ex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to run 12 different vanilla models, using RepeatedStratifiedKFold to perform cross validation. Using those results, the weaker models will be pruned and the best models will be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Log': LogisticRegression(), 'Knn': KNeighborsClassifier(), 'DT': DecisionTreeClassifier(random_state = 10), \n",
    "          'Gaussian': GaussianNB(), 'Multinomail': MultinomialNB(), 'LDA': LinearDiscriminantAnalysis(),\n",
    "          'LinearSVC': LinearSVC(max_iter = 1250, random_state = 10), 'SDGSVC': SGDClassifier(random_state = 10),  \n",
    "          'ADA': AdaBoostClassifier(random_state = 10), 'Bagging': BaggingClassifier(random_state = 10), \n",
    "          'Ridge': RidgeClassifier(random_state = 10), 'RF': RandomForestClassifier(random_state = 10)}\n",
    "\n",
    "# # getting results and model\n",
    "result_dict = test_models(X_train, y_train, models, n_jobs = 2)\n",
    "\n",
    "save_cv_results(result_dict, 'data_processing/models/VanillaResults_1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i[1] for i in result_dict.items()]\n",
    "model_names = [i for i in result_dict.keys()]\n",
    "plot_model_results(results, model_names, 'data_processing/models/VanillaResults_1.png', figure_title = 'Precision for Each Vanilla Model (version 1)', \n",
    "                   figsize = (13, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the results, `Log`, `KNN`, `Gaussian`, `LinearSVC`, `SDGSVC`, and `Ridge` are determined to be too weak, and they are purged from the list of models. \n",
    "\n",
    "With those out of the way, the cross validation is performed again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Knn': KNeighborsClassifier(), 'DT': DecisionTreeClassifier(random_state = 10), \n",
    "          'Multinomail': MultinomialNB(), 'LDA': LinearDiscriminantAnalysis(),\n",
    "          'ADA': AdaBoostClassifier(random_state = 10), 'Bagging': BaggingClassifier(random_state = 10), \n",
    "          'RF': RandomForestClassifier(random_state = 10)}\n",
    "\n",
    "\n",
    "# getting results and model\n",
    "result_dict = test_models(X_train, y_train, models, n_jobs = 2)\n",
    "\n",
    "save_cv_results(result_dict, 'data_processing/models/VanillaResults_2.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i[1] for i in result_dict.items()]\n",
    "model_names = [i for i in result_dict.keys()]\n",
    "plot_model_results(results, model_names, 'models/data_processing/VanillaResults_2.png', figure_title = 'Precision for Each Vanilla Model (version 2)', \n",
    "                   figsize = (13, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing this round, all but `ADA`, `Bagging`, and `RF` are purged. `DT` is purged as it's already very similar to `RF`\n",
    "\n",
    "Additionally, with the good performance of `ADA`, `GradientBoost` is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'ADA': AdaBoostClassifier(random_state = 10), 'Bagging': BaggingClassifier(random_state = 10), \n",
    "          'RF': RandomForestClassifier(random_state = 10), 'GradientBoost' : GradientBoostingClassifier(random_state = 10)}\n",
    "\n",
    "#create stacked model\n",
    "new_models = stacked_model(models)\n",
    "\n",
    "# getting results and model\n",
    "result_dict = test_models(X_train, y_train, new_models, n_jobs = 2)\n",
    "\n",
    "save_cv_results(result_dict, 'data_processing/models/VanillaResults_3.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i[1] for i in result_dict.items()]\n",
    "model_names = [i for i in result_dict.keys()]\n",
    "plot_model_results(results, model_names, 'data_processing/models/VanillaResults_3.png', figure_title = 'Precision for Each Vanilla Model (version 3)', \n",
    "                   figsize = (13, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the above four models, I moved on to [tuning them using Google cloud](GridSearch_CLOUD.ipynb)\n",
    "\n",
    "All results from the gridsearch were pickled and stored in [here](data_processing/models/Gridsearch_models), with the best results from all four pickled [here](data_processing/models/BestTunedClassifiers.p)\n",
    "\n",
    "Below are dictionaries of all of the best parameters for the models. All of the model names are stored in one list, while all of the other models are stored in another so that they can be examined together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = {\n",
    "    'ADA': AdaBoostClassifier(\n",
    "        algorithm = 'SAMME.R',\n",
    "        learning_rate = 1.5,\n",
    "        n_estimators = 1000,\n",
    "        random_state = 6\n",
    "    ),\n",
    "    \n",
    "    'Bagging': BaggingClassifier(\n",
    "        bootstrap = True,\n",
    "        bootstrap_features = True,\n",
    "        max_features = 0.5,\n",
    "        max_samples = 35,\n",
    "        n_estimators = 19,\n",
    "        random_state = 6,\n",
    "        warm_start = True\n",
    "    ),\n",
    "    \n",
    "    'RF': RandomForestClassifier(\n",
    "        bootstrap = False,\n",
    "        criterion = 'entropy',\n",
    "        max_depth = 30,\n",
    "        max_features = 'auto',\n",
    "        max_leaf_nodes = None,\n",
    "        max_samples = None,\n",
    "        min_samples_leaf = 15,\n",
    "        min_samples_split = 2,\n",
    "        n_estimators = 10,\n",
    "        random_state = 6\n",
    "    ),\n",
    "    \n",
    "    'GradientBoost' : GradientBoostingClassifier(\n",
    "        learning_rate = 1,\n",
    "        loss = 'deviance',\n",
    "        max_depth = 3,\n",
    "        min_samples_split = 15,\n",
    "        n_estimators = 100,\n",
    "        random_state = 6,\n",
    "        subsample = 1,\n",
    "    )}\n",
    "tuned_model_names = list(tuned_models.keys())\n",
    "tuned_models = list(tuned_models.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we'll also set four different variables corrisponding to each classifier to an arbitrary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADA_clf, Bag_clf, RF_clf, GB_clf = 0,0,0,0\n",
    "\n",
    "clfs = [ADA_clf, Bag_clf, RF_clf, GB_clf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a simple for loop, we can simply instantiate and fit each model using the parameters from the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    clfs[i] = tuned_models[i]\n",
    "    clfs[i].fit(X_train, y_train)\n",
    "    print(f\"Trained {tuned_model_names[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the models trained, the results can be viewed!\n",
    "\n",
    "At first, I'm going to just generate classification reports to review the results for eveything, but the main items I'm keeping an eye on are precision and accuracy.\n",
    "\n",
    "Ideally, the model would have a low number of projects misclassified as successful when they actually weren't, to help make sure false hopes aren't given to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = 0\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"\"\"Classification Report for {tuned_model_names[i]}\n",
    "-----\"\"\")\n",
    "    y_pred = clfs[i].predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, it looks like GradientBoost might be the best in terms of accuracy, however the precision for AdaBoost and GradientBoost are matched for precision. So next, I'm going to be looking at both the precision score and accuracy score for everything to get a more detailed number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    y_pred = clfs[i].predict(X_test)\n",
    "    print(f\"\"\"Precision Score for {tuned_model_names[i]} : {precision_score(y_test, y_pred)}\"\"\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(f\"\"\"Accuracy Score for {tuned_model_names[i]} : {clfs[i].score(X_test, y_test)}\"\"\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, we can see that AdaBoost and GradientBoost are very close on both precision and accuracy, but GradientBoost is at least a little bit more accurate than AdaBoost. To be sure, I went to check the confusion matrix for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(f\"\"\"Confusion Matrix for {tuned_model_names[i]}\n",
    "-----\"\"\")\n",
    "    y_pred = clfs[i].predict(X_test)\n",
    "#     plot_confusion_matrix(clfs[i], X_test, y_test, cmap = \"Blues\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, I can once again confirm GradientBoost has the lowest number of false positives, on top of having the most correct guesses for both true positives and true negatives!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tuned GradientBoost model being just barely 73% accurate, it's not ideal. However, it's enough to be helpful and augment what the project creator already knows, and give some idea of ideal date ranges, project lenghts, and goals.\n",
    "As long as the amount of risk is considered, and the 1/4 chance that the model is incorrect, then it can be considered to be helpful, as long as it is not depended upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "- Test other algorithms\n",
    "    - While working on this project, I was unable to get XGBoost to work. If possible, one of my next steps would be to troubleshoot this issue and see if it performs better than the other models I have tested\n",
    "    - As well, I would like to research other classifier algorithms that may be used to further improve performance\n",
    "    - I would like to test using a stacked classifier to improve overall model performance\n",
    "   \n",
    "- Test creating models for each parent category\n",
    "    - This is based off of my theory that making a model for each parent category, and using the child categories as a feature may improve the performance of the models. This will be very time consuming due to the number of parent categories. \n",
    "    \n",
    "- Alter number of features used \n",
    "    - I would like to alter the number of features used, and see if removing some may improve overall model performance.\n",
    "\n",
    "- Create a separate NLP algorithm that analyzes blurb data and titlesto analyze whether or not the words used may draw more attention, and therefore, more backers.\n",
    "    - Would take a lot of preprocessing and manual addition of words, as well as manual removal of stopwords. Some types of items may not be recognized as \"real\" when stopwords and non-english words are removed, which may cause issues and lower accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KS_Env",
   "language": "python",
   "name": "ks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
